{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load\n",
    "df = np.load('mnist.npz') \n",
    "#split\n",
    "x_test  = df['x_test']\n",
    "y_test  = df['y_test']\n",
    "x_train = df['x_train']\n",
    "y_train = df['y_train']\n",
    "x_train = x_train/255\n",
    "x_test  = x_test/255\n",
    "x_train = x_train.reshape(-1,784)\n",
    "x_test  = x_test.reshape(-1,784)\n",
    "bias_x_train  = -np.ones((60000,1))\n",
    "x_train = np.concatenate((x_train, bias_x_train), axis = 1)\n",
    "bias_x_test  = -np.ones((10000,1))\n",
    "x_test  = np.concatenate((x_test, bias_x_test), axis = 1)\n",
    "N = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x_train[:20000]\n",
    "y = y_train[:20000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layers:\n",
    "    def forward(self, X, w_1, w_2):\n",
    "        bias_h  = -np.ones((1,))\n",
    "        v_1 = np.matmul(w_1, X) # data*weights+bias\n",
    "        v_1_concat = np.concatenate((v_1, bias_h), axis = 0)\n",
    "        h_1 = self.activation(v_1_concat)\n",
    "        v_2 = np.matmul(w_2, h_1)\n",
    "        return v_1_concat, h_1, v_2, v_1\n",
    "    \n",
    "    def forward_2(self, X, w_1, w_2):\n",
    "        bias_h  = -np.ones((1,))\n",
    "        v_1 = np.matmul(w_1, X) # data*weights+bias\n",
    "        v_1_concat = np.concatenate((v_1, bias_h), axis = 0)\n",
    "        h_1 = self.activation_2(v_1_concat)\n",
    "        v_2 = np.matmul(w_2, h_1)\n",
    "        return v_1_concat, h_1, v_2, v_1\n",
    "\n",
    "    def activation(self, x):   # used step here\n",
    "        y = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        return y\n",
    "    \n",
    "    def activation_2(self, x):   # used step here\n",
    "        y = np.maximum(0, x)\n",
    "        return y\n",
    "    \n",
    "    def activation_3(self, x):   # used step here\n",
    "        y = 1/(1 + np.exp(-x))\n",
    "        return y\n",
    "    \n",
    "    def derivative(self,x):\n",
    "        y = 1 - ((np.exp(2*x) - 1)/(np.exp(2*x) + 1))**2\n",
    "        return y\n",
    "    \n",
    "    def derivative_2(self,x):\n",
    "        y = x > 0\n",
    "        return y\n",
    "    \n",
    "    def derivative_3(self,x):\n",
    "        y = (1/(1 + np.exp(-x)))*(1 - 1/(1 + np.exp(-x)))\n",
    "        return y\n",
    "    \n",
    "    def predict(self, X, w_1, w_2):     \n",
    "        z = self.forward(X, w_1, w_2)[2] #forward on the data\n",
    "        y = self.activation(z) #apply the activation function\n",
    "        return y\n",
    "    \n",
    "    def predict_2(self, X, w_1, w_2):     \n",
    "        z = self.forward_2(X, w_1, w_2)[2] #forward on the data\n",
    "        y = self.activation_3(z) #apply the activation function\n",
    "        return y\n",
    "\n",
    "    def gradient(self, X, y, w_1, w_2, w_1_concat, w_2_concat, N, l_c):\n",
    "        \n",
    "        h_1_reshaped       = np.reshape(self.forward(X, w_1_concat, w_2_concat)[1],(N+1, 1))\n",
    "        delta_w_2          = self.error(X, y)*self.derivative(self.forward(X, w_1_concat, w_2_concat)[2])\n",
    "        delta_w_2_reshaped = np.reshape(delta_w_2,(10, 1))\n",
    "        update_w_2         = np.matmul(delta_w_2_reshaped, np.transpose(h_1_reshaped))\n",
    "\n",
    "        delta_w_1          = self.derivative(self.forward(X, w_1_concat, w_2_concat)[3]) * np.matmul(np.transpose(w_2), delta_w_2)\n",
    "        delta_w_1_reshaped = np.reshape(delta_w_1,(N, 1))\n",
    "        X                  = np.reshape(X, (785, 1))\n",
    "        update_w_1         = np.matmul(delta_w_1_reshaped, np.transpose(X))\n",
    "\n",
    "        w_1_concat += l_c*update_w_1\n",
    "        w_2_concat += l_c*update_w_2\n",
    "        w_1 = w_1_concat[:,:-1]\n",
    "        w_2 = w_2_concat[:,:-1]\n",
    "        \n",
    "        return w_1, w_2, w_1_concat, w_2_concat\n",
    "    \n",
    "    def gradient_2(self, X, y, w_1, w_2, w_1_concat, w_2_concat, N, l_c):\n",
    "        \n",
    "        h_1_reshaped       = np.reshape(self.forward_2(X, w_1_concat, w_2_concat)[1],(N+1,1))\n",
    "        delta_w_2          = self.error_2(X, y)*self.derivative_3(self.forward_2(X, w_1_concat, w_2_concat)[2])\n",
    "        delta_w_2_reshaped = np.reshape(delta_w_2,(10,1))\n",
    "        update_w_2         = np.matmul(delta_w_2_reshaped, np.transpose(h_1_reshaped))\n",
    "\n",
    "        delta_w_1          = self.derivative_2(self.forward_2(X, w_1_concat, w_2_concat)[3]) * np.dot(delta_w_2, w_2)\n",
    "        delta_w_1_reshaped = np.reshape(delta_w_1,(N,1))\n",
    "        X                  = np.reshape(X, (785,1))\n",
    "        update_w_1         = np.matmul(delta_w_1_reshaped, np.transpose(X))\n",
    "\n",
    "        w_1_concat += l_c*update_w_1\n",
    "        w_2_concat += l_c*update_w_2\n",
    "        w_1 = w_1_concat[:,:-1]\n",
    "        w_2 = w_2_concat[:,:-1]\n",
    "            \n",
    "        return w_1, w_2, w_1_concat, w_2_concat\n",
    "                       \n",
    "    def pre_assign(self, y):\n",
    "        y_label=-np.ones((10,))\n",
    "        y_label[y]=1\n",
    "        return y_label\n",
    "    \n",
    "    def pre_assign_2(self, y):\n",
    "        y_label = np.zeros((10,))\n",
    "        y_label[y] = 1\n",
    "        return y_label\n",
    "    \n",
    "    def error(self, X, y):\n",
    "        dif = self.pre_assign(y) - self.predict(X, w_1_concat, w_2_concat)\n",
    "        return dif\n",
    "    \n",
    "    def error_2(self, X, y):\n",
    "        dif = self.pre_assign_2(y) - self.predict_2(X, w_1_concat, w_2_concat)\n",
    "        return dif\n",
    "    \n",
    "    def error_reg(self, X, y, w_1_concat, w_2_concat, coef):\n",
    "        dif = self.pre_assign_2(y) - self.predict_2(X, w_1_concat, w_2_concat) + coef* np.sum(np.square(w_2_concat))\n",
    "        return dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 \n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Training Error    = 0.1882 \n",
      "Training Accuracy = 94.8300 \n",
      "Completation Time = 157.69 seconds\n",
      "\n",
      "Case 1 \n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Testing Error    = 0.2647 \n",
      "Testing Accuracy = 92.7600 \n",
      "Completation Time = 1.08 seconds\n",
      "\n",
      "Case 1 \n",
      "N = 300\n",
      "Learning Rate     = 0.05 \n",
      "Training Error    = 0.7236 \n",
      "Training Accuracy = 79.5700 \n",
      "Completation Time = 131.58 seconds\n",
      "\n",
      "Case 1 \n",
      "N = 300\n",
      "Learning Rate     = 0.05 \n",
      "Testing Error    = 0.7639 \n",
      "Testing Accuracy = 78.3800 \n",
      "Completation Time = 0.99 seconds\n",
      "\n",
      "Case 1 \n",
      "N = 300\n",
      "Learning Rate     = 0.09 \n",
      "Training Error    = 1.6337 \n",
      "Training Accuracy = 29.0500 \n",
      "Completation Time = 124.18 seconds\n",
      "\n",
      "Case 1 \n",
      "N = 300\n",
      "Learning Rate     = 0.09 \n",
      "Testing Error    = 1.6550 \n",
      "Testing Accuracy = 28.3300 \n",
      "Completation Time = 0.92 seconds\n",
      "\n",
      "Case 2 \n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Training Error    = 0.0632 \n",
      "Training Accuracy = 93.0000 \n",
      "Completation Time = 116.40 seconds\n",
      "\n",
      "Case 2 \n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Testing Error    = 1.8886 \n",
      "Testing Accuracy = 86.4800 \n",
      "Completation Time = 0.93 seconds\n",
      "\n",
      "Case 2 \n",
      "N = 300\n",
      "Learning Rate     = 0.05 \n",
      "Training Error    = 0.0268 \n",
      "Training Accuracy = 97.1300 \n",
      "Completation Time = 114.82 seconds\n",
      "\n",
      "Case 2 \n",
      "N = 300\n",
      "Learning Rate     = 0.05 \n",
      "Testing Error    = 6.8994 \n",
      "Testing Accuracy = 74.7700 \n",
      "Completation Time = 1.02 seconds\n",
      "\n",
      "Case 2 \n",
      "N = 300\n",
      "Learning Rate     = 0.09 \n",
      "Training Error    = 0.0241 \n",
      "Training Accuracy = 97.4800 \n",
      "Completation Time = 125.87 seconds\n",
      "\n",
      "Case 2 \n",
      "N = 300\n",
      "Learning Rate     = 0.09 \n",
      "Testing Error    = 11.5002 \n",
      "Testing Accuracy = 71.5900 \n",
      "Completation Time = 1.05 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-ba1dce9ac57e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m                         \u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_c\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m                 \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-4ce60337c444>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, X, y, w_1, w_2, w_1_concat, w_2_concat, N, l_c)\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mdelta_w_2\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[0mdelta_w_2_reshaped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_w_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mupdate_w_2\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_w_2_reshaped\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_1_reshaped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mdelta_w_1\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelta_w_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_list   = [300, 500, 1000]\n",
    "l_c_list = [0.01, 0.05, 0.09]\n",
    "for n in N_list:\n",
    "    N = n\n",
    "    for case in range(1,3):\n",
    "        for i in l_c_list:\n",
    "            l_c = i\n",
    "            if case == 1:\n",
    "                w_1 = np.random.normal(scale=0.01, size = (N, 784))\n",
    "                b_1 = np.zeros((N,1))\n",
    "                w_2 = np.random.normal(scale=0.01, size = (10, N))\n",
    "                b_2 = np.zeros((10,1))\n",
    "\n",
    "                w_1_concat = np.concatenate((w_1, b_1), axis = 1)\n",
    "                w_2_concat = np.concatenate((w_2, b_2), axis = 1)\n",
    "                \n",
    "                start = time.time()\n",
    "                \n",
    "                X = x_train[:10000]\n",
    "                y = y_train[:10000]\n",
    "                \n",
    "                for j in range(5):\n",
    "                    for i in range(len(X)):\n",
    "                        w_1, w_2, w_1_concat, w_2_concat = layers().gradient(X[i], y[i], w_1, w_2, w_1_concat, w_2_concat, N, l_c)\n",
    "  \n",
    "                err = 0\n",
    "                predictions = np.zeros((len(y),10))\n",
    "                results = np.zeros(len(y))\n",
    "\n",
    "                for i in range(len(X)):\n",
    "                    predictions[i] = layers().predict(X[i], w_1_concat, w_2_concat)\n",
    "                    results[i]     = np.argmax(predictions[i])\n",
    "                    err += np.sum(np.square(predictions[i] - layers().pre_assign(y[i]))/2, axis = 0)\n",
    "                err /= 10000\n",
    "                accuracy_train = 100*(1 - np.sum(y != results)/len(y))\n",
    "                end = time.time()\n",
    "                time_passed = end - start\n",
    "                print('Case 1 \\nN = %.d\\nLearning Rate     = %.2f \\nTraining Error    = %.4f \\nTraining Accuracy = %.4f \\nCompletation Time = %.2f seconds\\n' % (N, l_c, err, accuracy_train, time_passed))\n",
    "                \n",
    "                X = x_test\n",
    "                y = y_test\n",
    "                \n",
    "                err = 0\n",
    "                predictions = np.zeros((len(y),10))\n",
    "                results = np.zeros(len(y))\n",
    "                start = time.time()\n",
    "                for i in range(len(X)):\n",
    "                    predictions[i] = layers().predict(X[i], w_1_concat, w_2_concat)\n",
    "                    results[i]     = np.argmax(predictions[i])\n",
    "                    err += np.sum(np.square(predictions[i] - layers().pre_assign(y[i]))/2, axis = 0)\n",
    "                err /= 10000\n",
    "                accuracy_train = 100*(1 - np.sum(y != results)/len(y))\n",
    "                end = time.time()\n",
    "                time_passed = end - start\n",
    "                print('Case 1 \\nN = %.d\\nLearning Rate     = %.2f \\nTesting Error    = %.4f \\nTesting Accuracy = %.4f \\nCompletation Time = %.2f seconds\\n' % (N, l_c, err, accuracy_train, time_passed))\n",
    "\n",
    "            else:\n",
    "                w_1 = np.random.normal(scale=0.01, size = (N, 784))\n",
    "                b_1 = np.zeros((N,1))\n",
    "                w_2 = np.random.normal(scale=0.01, size = (10, N))\n",
    "                b_2 = np.zeros((10,1))\n",
    "\n",
    "                w_1_concat = np.concatenate((w_1, b_1), axis = 1)\n",
    "                w_2_concat = np.concatenate((w_2, b_2), axis = 1)\n",
    "                \n",
    "                start = time.time()\n",
    "                \n",
    "                X = x_train[:10000]\n",
    "                y = y_train[:10000]\n",
    "                \n",
    "                for i in range(5):\n",
    "                    for i in range(len(X)):\n",
    "                        w_1, w_2, w_1_concat, w_2_concat = layers().gradient_2(X[i], y[i], w_1, w_2, w_1_concat, w_2_concat, N, l_c)\n",
    "                err = 0\n",
    "                predictions = np.zeros((len(y),10))\n",
    "                results = np.zeros(len(y))\n",
    "\n",
    "                for i in range(len(X)):\n",
    "                    predictions[i] = layers().predict_2(X[i], w_1_concat, w_2_concat)\n",
    "                    results[i]     = np.argmax(predictions[i])\n",
    "                    err += np.sum(np.square(predictions[i] - layers().pre_assign_2(y[i]))/2, axis = 0)\n",
    "                err /= 10000\n",
    "                accuracy_train = 100*(1 - np.sum(y != results)/len(y))\n",
    "                end = time.time()\n",
    "                time_passed = end - start\n",
    "                print('Case 2 \\nN = %.d\\nLearning Rate     = %.2f \\nTraining Error    = %.4f \\nTraining Accuracy = %.4f \\nCompletation Time = %.2f seconds\\n' % (N, l_c, err, accuracy_train, time_passed))\n",
    "                \n",
    "                X = x_test\n",
    "                y = y_test\n",
    "                \n",
    "                err = 0\n",
    "                predictions = np.zeros((len(y),10))\n",
    "                results = np.zeros(len(y))\n",
    "                start = time.time()\n",
    "                for i in range(len(X)):\n",
    "                    predictions[i] = layers().predict(X[i], w_1_concat, w_2_concat)\n",
    "                    results[i]     = np.argmax(predictions[i])\n",
    "                    err += np.sum(np.square(predictions[i] - layers().pre_assign(y[i]))/2, axis = 0)\n",
    "                err /= 10000\n",
    "                accuracy_train = 100*(1 - np.sum(y != results)/len(y))\n",
    "                end = time.time()\n",
    "                time_passed = end - start\n",
    "                print('Case 2 \\nN = %.d\\nLearning Rate     = %.2f \\nTesting Error    = %.4f \\nTesting Accuracy = %.4f \\nCompletation Time = %.2f seconds\\n' % (N, l_c, err, accuracy_train, time_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layers_2:\n",
    "    def forward(self, X, w_1, w_2):\n",
    "        bias_h  = -np.ones((1, X.shape[1]))\n",
    "        v_1 = np.matmul(w_1, X) # data*weights+bias\n",
    "        v_1_concat = np.concatenate((v_1, bias_h), axis = 0)\n",
    "        h_1 = self.activation(v_1_concat)\n",
    "        v_2 = np.matmul(w_2, h_1)\n",
    "        return v_1_concat, h_1, v_2, v_1\n",
    "\n",
    "    def activation(self, x):   # used step here\n",
    "        y = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        return y\n",
    "    \n",
    "    def derivative(self,x):\n",
    "        y = 1 - ((np.exp(2*x) - 1)/(np.exp(2*x) + 1))**2\n",
    "        return y\n",
    "    \n",
    "    def predict(self, X, w_1, w_2):     \n",
    "        z = self.forward(X, w_1, w_2)[2] #forward on the data\n",
    "        y = self.activation(z) #apply the activation function\n",
    "        return y\n",
    "\n",
    "    def gradient(self, X, y, w_1, w_2, w_1_concat, w_2_concat, N, l_c, batch_size):\n",
    "        \n",
    "        h_1                = self.forward(X, w_1_concat, w_2_concat)[1]\n",
    "        delta_w_2          = self.error(X, y)*self.derivative(self.forward(X, w_1_concat, w_2_concat)[2])\n",
    "        update_w_2         = np.matmul(delta_w_2, np.transpose(h_1))/batch_size\n",
    "\n",
    "        delta_w_1          = self.derivative(self.forward(X, w_1_concat, w_2_concat)[3]) * np.matmul(np.transpose(w_2), delta_w_2)\n",
    "        update_w_1         = np.matmul(delta_w_1, np.transpose(X))/batch_size\n",
    "\n",
    "        w_1_concat += l_c*update_w_1\n",
    "        w_2_concat += l_c*update_w_2\n",
    "        w_1 = w_1_concat[:,:-1]\n",
    "        w_2 = w_2_concat[:,:-1]\n",
    "        \n",
    "        return w_1, w_2, w_1_concat, w_2_concat\n",
    "                       \n",
    "    def pre_assign(self, X, y):\n",
    "        y_label=-np.ones((10, X.shape[1]))\n",
    "        for i in range(len(y)):\n",
    "            y_label[:,i][y[i]] = 1\n",
    "        return y_label\n",
    "    \n",
    "    def pre_assign_2(self, y):\n",
    "        y_label=-np.ones((10, ))\n",
    "        y_label[y] = 1\n",
    "        return y_label\n",
    "    \n",
    "    def error(self, X, y):\n",
    "        dif = self.pre_assign(X, y) - self.predict(X, w_1_concat, w_2_concat)\n",
    "        return dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 \n",
      "Mini Batch Size = 10\n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Training Error    = 0.0476 \n",
      "Training Accuracy = 98.1889 \n",
      "Completation Time = 313.00 seconds\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-3d107d19428d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m             \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m             \u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayers_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-db15a839b85c>\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, X, y, w_1, w_2, w_1_concat, w_2_concat, N, l_c, batch_size)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml_c\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mh_1\u001b[0m                \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mdelta_w_2\u001b[0m          \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mderivative\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2_concat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mupdate_w_2\u001b[0m         \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta_w_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-db15a839b85c>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, w_1, w_2)\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m         \u001b[0mbias_h\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m         \u001b[0mv_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# data*weights+bias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mv_1_concat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias_h\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mh_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv_1_concat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size_list = [10, 50, 100]\n",
    "l_c = 0.01\n",
    "case = 1\n",
    "for batch_size in batch_size_list:\n",
    "    w_1 = np.random.normal(scale=0.01, size = (N, 784))\n",
    "    b_1 = np.zeros((N,1))\n",
    "    w_2 = np.random.normal(scale=0.01, size = (10, N))\n",
    "    b_2 = np.zeros((10,1))\n",
    "\n",
    "    w_1_concat = np.concatenate((w_1, b_1), axis = 1)\n",
    "    w_2_concat = np.concatenate((w_2, b_2), axis = 1)\n",
    "    start= time.time()\n",
    "    for i in range(100):\n",
    "        for i in range(len(X)//batch_size):\n",
    "            a = np.transpose(X[batch_size*i:batch_size*(i+1)])\n",
    "            b = np.transpose(y[batch_size*i:batch_size*(i+1)])\n",
    "            w_1, w_2, w_1_concat, w_2_concat = layers_2().gradient(a, b , w_1, w_2, w_1_concat, w_2_concat, N, l_c, batch_size)\n",
    "    err = 0\n",
    "    predictions = np.zeros((len(y),10))\n",
    "    results = np.zeros(len(y))\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        a = np.reshape(X[i], (785, 1))\n",
    "        predictions[i] = np.reshape(layers_2().predict(a, w_1_concat, w_2_concat), (10,))\n",
    "        results[i]     = np.argmax(predictions[i])\n",
    "        err += np.sum(np.square(predictions[i] - layers_2().pre_assign_2(y[i]))/2, axis = 0)\n",
    "\n",
    "    err /= 10000\n",
    "    accuracy_train = 100*(1 - np.sum(y != results)/len(y))\n",
    "    end = time.time()\n",
    "    time_passed = end - start\n",
    "    print('Case 1 \\nMini Batch Size = %.d\\nN = %.d\\nLearning Rate     = %.2f \\nTraining Error    = %.4f \\nTraining Accuracy = %.4f \\nCompletation Time = %.2f seconds\\n' % (batch_size, N, l_c, err, accuracy_train, time_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class layers_3:\n",
    "    def forward(self, X, w_1, w_2):\n",
    "        bias_h  = -np.ones((1, X.shape[1]))\n",
    "        v_1 = np.matmul(w_1, X) # data*weights+bias\n",
    "        v_1_concat = np.concatenate((v_1, bias_h), axis = 0)\n",
    "        h_1 = self.activation(v_1_concat)\n",
    "        v_2 = np.matmul(w_2, h_1)\n",
    "        return v_1_concat, h_1, v_2, v_1\n",
    "\n",
    "    def activation(self, x):   # used step here\n",
    "        y = (np.exp(2*x) - 1)/(np.exp(2*x) + 1)\n",
    "        return y\n",
    "    \n",
    "    def derivative(self,x):\n",
    "        y = 1 - ((np.exp(2*x) - 1)/(np.exp(2*x) + 1))**2\n",
    "        return y\n",
    "    \n",
    "    def predict(self, X, w_1, w_2):     \n",
    "        z = self.forward(X, w_1, w_2)[2] #forward on the data\n",
    "        y = self.activation(z) #apply the activation function\n",
    "        return y\n",
    "\n",
    "    def gradient(self, X, y, w_1, w_2, w_1_concat, w_2_concat, N, l_c, coef):\n",
    "        \n",
    "        h_1                = self.forward(X, w_1_concat, w_2_concat)[1]\n",
    "        delta_w_2          = self.error(X, y)*self.derivative(self.forward(X, w_1_concat, w_2_concat)[2])\n",
    "        update_w_2         = np.matmul(delta_w_2, np.transpose(h_1))\n",
    "\n",
    "        delta_w_1          = self.derivative(self.forward(X, w_1_concat, w_2_concat)[3]) * np.matmul(np.transpose(w_2), delta_w_2)\n",
    "        update_w_1         = np.matmul(delta_w_1, np.transpose(X))\n",
    "\n",
    "        w_1_concat += l_c*update_w_1\n",
    "        w_2_concat += l_c*update_w_2\n",
    "        w_1 = w_1_concat[:,:-1]\n",
    "        w_2 = w_2_concat[:,:-1]\n",
    "        \n",
    "        return w_1, w_2, w_1_concat, w_2_concat\n",
    "                       \n",
    "    def pre_assign(self, X, y):\n",
    "        y_label=-np.ones((10, X.shape[1]))\n",
    "        for i in range(len(y)):\n",
    "            y_label[:,i][y[i]] = 1\n",
    "        return y_label\n",
    "    \n",
    "    def pre_assign_2(self, y):\n",
    "        y_label=-np.ones((10, ))\n",
    "        y_label[y] = 1\n",
    "        return y_label\n",
    "    \n",
    "    def error(self, X, y):\n",
    "        dif = self.pre_assign(X, y) - self.predict(X, w_1_concat, w_2_concat) + coef* np.sum(np.square(w_2_concat))\n",
    "        return dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Case 1 \n",
      "Mini Batch Size = 10\n",
      "Lambda = 0.001\n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Training Error    = 0.3703 \n",
      "Training Accuracy = 88.7889 \n",
      "Completation Time = 3.40 seconds\n",
      "\n",
      "Case 1 \n",
      "Mini Batch Size = 10\n",
      "Lambda = 0.010\n",
      "N = 300\n",
      "Learning Rate     = 0.01 \n",
      "Training Error    = 0.3981 \n",
      "Training Accuracy = 89.1222 \n",
      "Completation Time = 6.81 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "l_c = 0.01\n",
    "case = 1\n",
    "lambda_list = [0.001, 0.01]\n",
    "start= time.time()\n",
    "for coef in lambda_list:\n",
    "    w_1 = np.random.normal(scale=0.01, size = (N, 784))\n",
    "    b_1 = np.zeros((N,1))\n",
    "    w_2 = np.random.normal(scale=0.01, size = (10, N))\n",
    "    b_2 = np.zeros((10,1))\n",
    "\n",
    "    w_1_concat = np.concatenate((w_1, b_1), axis = 1)\n",
    "    w_2_concat = np.concatenate((w_2, b_2), axis = 1)\n",
    "    for i in range(1):\n",
    "        for i in range(len(X)//10):\n",
    "            a = np.transpose(X[10*i:10*(i+1)])\n",
    "            b = np.transpose(y[10*i:10*(i+1)])\n",
    "            w_1, w_2, w_1_concat, w_2_concat = layers_3().gradient(a, b , w_1, w_2, w_1_concat, w_2_concat, N, l_c, coef )\n",
    "    err = 0\n",
    "    predictions = np.zeros((len(y),10))\n",
    "    results = np.zeros(len(y))\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        a = np.reshape(X[i], (785, 1))\n",
    "        predictions[i] = np.reshape(layers_3().predict(a, w_1_concat, w_2_concat), (10,))\n",
    "        results[i]     = np.argmax(predictions[i])\n",
    "        err += np.sum(np.square(predictions[i] - layers_3().pre_assign_2(y[i]))/2, axis = 0)\n",
    "\n",
    "    err /= 10000\n",
    "    accuracy_train = 100*(1 - np.sum(y != results)/len(y))\n",
    "    end = time.time()\n",
    "    time_passed = end - start\n",
    "    print('Case 1 \\nMini Batch Size = %.d\\nLambda = %.3f\\nN = %.d\\nLearning Rate     = %.2f \\nTraining Error    = %.4f \\nTraining Accuracy = %.4f \\nCompletation Time = %.2f seconds\\n' % (batch_size, coef, N, l_c, err, accuracy_train, time_passed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
